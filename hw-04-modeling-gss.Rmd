---
title: "HW 04 - Modeling the GSS"
author: "Yaoxing Qian"
output: 
---

```{r include = FALSE}
knitr::opts_chunk$set(
  eval = TRUE,
  out.width = "80%",
  fig.asp = 0.618,
  fig.width = 10,
  dpi = 300
)
```

```{r photo, fig.margin = TRUE, echo = FALSE, fig.width = 3, fig.cap = "Photo Mauro Mora on Unsplash", eval = TRUE}

knitr::include_graphics("img/mauro-mora-31-pOduwZGE-unsplash.jpg")
```

In this assignment we continue our exploration of the 2016 GSS dataset from the previous homework.

# Getting started

Go to the course GitHub organization and locate your homework repo, clone it in RStudio and open the R Markdown document.
Knit the document to make sure it compiles without errors.

## Warm up

Before we introduce the data, let's warm up with some simple exercises.
Update the YAML of your R Markdown file with your information, knit, commit, and push your changes.
Make sure to commit with a meaningful commit message.
Then, go to your repo on GitHub and confirm that your changes are visible in your Rmd **and** md files.
If anything is missing, commit and push again.

## Packages

We'll use the **tidyverse** package for much of the data wrangling and visualisation, the **tidymodels** package for modeling and inference, and the data lives in the **dsbox** package.
These packages are already installed for you.
You can load them by running the following in your Console:

```{r load-packages, message = FALSE, eval = TRUE}
library(tidyverse)
library(tidymodels)
library(dsbox)
```

## Data

The data can be found in the **dsbox** package, and it's called `gss16`.
Since the dataset is distributed with the package, we don't need to load it separately; it becomes available to us when we load the package.
You can find out more about the dataset by inspecting its documentation, which you can access by running `?gss16` in the Console or using the Help menu in RStudio to search for `gss16`.
You can also find this information [here](https://rstudio-education.github.io/dsbox/reference/gss16.html).

# Exercises

## Scientific research

In this section we're going to build a model to predict whether someone agrees or doesn't agree with the following statement:

> Even if it brings no immediate benefits, scientific research that advances the frontiers of knowledge is necessary and should be supported by the federal government.

The responses to the question on the GSS about this statement are in the `advfront` variable.

```{marginfigure}
It's important that you don't recode the NAs, just the remaining levels.
```

1.  Re-level the `advfront` variable such that it has two levels: `Strongly agree` and "`Agree"` combined into a new level called `agree` and the remaining levels (except `NA`s) combined into "`Not agree"`. Then, re-order the levels in the following order: `"Agree"` and `"Not agree"`. Finally, `count()` how many times each new level appears in the `advfront` variable.

```{marginfigure}
You can do this in various ways. One option is to use the `str_detect()` function to detect the existence of words like liberal or conservative. Note that these sometimes show up with lowercase first letters and sometimes with upper case first letters. To detect either in the `str_detect()` function, you can use "[Ll]iberal" and "[Cc]onservative". But feel free to solve the problem however you like, this is just one option!
```
```{r}
library(tidyverse)
library(dsbox)  

gss16 %>%
  filter(!is.na(advfront)) %>%
  mutate(
    advfront2 = case_when(
      advfront %in% c("Strongly agree", "Agree") ~ "Agree",
      TRUE                                      ~ "Not agree"
    ),
    advfront2 = factor(advfront2, levels = c("Agree", "Not agree"))
  ) %>%
  count(advfront2)
```
***A total of 1,159 respondents supported the proposal, while 226 respondents opposed it.***

2.  Combine the levels of the `polviews` variable such that levels that have the word "liberal" in them are lumped into a level called `"Liberal"` and those that have the word conservative in them are lumped into a level called `"Conservative"`. Then, re-order the levels in the following order: `"Conservative"` , `"Moderate"`, and `"Liberal"`. Finally, `count()` how many times each new level appears in the `polviews` variable.

```{r}
gss16 %>%
  filter(!is.na(polviews)) %>%
  mutate(
    pol2 = case_when(
      str_detect(polviews, "[Ll]iberal")      ~ "Liberal",
      str_detect(polviews, "[Cc]onservative") ~ "Conservative",
      TRUE                                     ~ "Moderate"
    ),
    pol2 = factor(pol2, levels = c("Conservative", "Moderate", "Liberal"))
  ) %>%
  count(pol2)
```

***A total of 928 respondents identified themselves as conservative, 1,032 identified themselves as moderate, and 796 identified themselves as liberal.***


3.  Create a new data frame called `gss16_advfront` that includes the variables `advfront`, `educ`, `polviews`, and `wrkstat`. Then, use the `drop_na()` function to remove rows that contain `NA`s from this new data frame. Sample code is provided below.

```{r eval=FALSE}
gss16_advfront <- gss16 %>%
  select(___, ___, ___, ___) %>%
  drop_na()
```

```{r 3-gss16_advfront, message=FALSE, warning=FALSE}
library(tidyverse)
library(dsbox)

gss16_advfront <- gss16 %>%
  filter(!is.na(advfront)) %>%
  mutate(advfront = case_when(
    advfront %in% c("Strongly agree", "Agree") ~ "Agree",
    TRUE                                      ~ "Not agree"
  )) %>%
  select(advfront, educ, polviews, wrkstat) %>%
  drop_na()
```
***The results show that gss16_advfront contains a total of 1328 observations***

4.  Split the data into training (75%) and testing (25%) data sets. Make sure to set a seed before you do the `initial_split()`. Call the training data `gss16_train` and the testing data `gss16_test`. Sample code is provided below. Use these specific names to make it easier to follow the rest of the instructions.

```{r eval=FALSE}
set.seed(___)
gss16_split <- initial_split(gss16_advfront)
gss16_train <- training(gss16_split)
gss16_test  <- testing(gss16_split)
```

```{r 4-split, message=FALSE, warning=FALSE}
library(tidymodels)

set.seed(2025)                          
gss16_split <- initial_split(gss16_advfront, prop = 0.75)

gss16_train <- training(gss16_split)   
gss16_test  <- testing(gss16_split)      


nrow(gss16_train) 
nrow(gss16_test)  
```
***There are 996 records in the training set and 332 records in the test set***

5.  Create a recipe with the following steps for predicting `advfront` from `polviews`, `wrkstat`, and `educ`.
    Name this recipe `gss16_rec_1`.
    (We'll create one more recipe later, that's why we're naming this recipe `_1`.) Sample code is provided below.

    -   `step_other()` to pool values that occur less than 10% of the time (`threshold = 0.10`) in the `wrkstat` variable into `"Other"`.

    -   `step_dummy()` to create dummy variables for `all_nominal()` variables that are predictors, i.e. `all_predictors()`

```{r eval=FALSE}
gss16_rec_1 <- recipe(___ ~ ___, data = ___) %>%
  step_other(wrkstat, threshold = ___, other = "Other") %>%
  step_dummy(all_nominal(), -all_outcomes())
```
```{r}
library(tidymodels)

gss16_rec_1 <- recipe(advfront ~ polviews + wrkstat + educ, data = gss16_train) %>%
  step_other(wrkstat, threshold = 0.10, other = "Other") %>%
  step_dummy(all_nominal(), -all_outcomes())

gss16_rec_1
```

***I merged the categories that appeared less than 10% in wrkstat into "Other", and then treated the three predictor variables polviews, wrkstat, and educ as dummy variables.***

6.  Specify a logistic regression model using `"glm"` as the engine. Name this specification `gss16_spec`. Sample code is provided below.

```{r eval=FALSE}
gss16_spec <- ___() %>%
  set_engine("___")
```

```{r 6-spec, message=FALSE, warning=FALSE}
library(tidymodels)

gss16_spec <- logistic_reg() %>%
  set_engine("glm")

gss16_spec
```

***I created a logistic regression model using the logistic_reg() function and used set_engine("glm") to specify that R’s built-in glm() be used for fitting.***

7.  Build a workflow that uses the recipe you defined (`gss16_rec`) and the model you specified (`gss16_spec`). Name this workflow `gss16_wflow_1`. Sample code is provided below.

```{r eval=FALSE}
gss16_wflow_1 <- workflow() %>%
  add_model(___) %>%
  add_recipe(___)
```

```{r 7-workflow, message=FALSE, warning=FALSE}
library(tidymodels)

gss16_wflow_1 <- workflow() %>%
  add_model(gss16_spec) %>%      
  add_recipe(gss16_rec_1)       

gss16_wflow_1
```
***I created a shell and put the logistic regression model and preprocessing pipeline into it.***

8.  Perform 5-fold cross validation.
    specifically,

    -   split the training data into 5 folds (don't forget to set a seed first!),

    -   apply the workflow you defined earlier to the folds with `fit_resamples()`, and

    -   `collect_metrics()` and comment on the consistency of metrics across folds (you can get the area under the ROC curve and the accuracy for each fold by setting `summarize = FALSE` in `collect_metrics()`)

    -   report the average area under the ROC curve and the accuracy for all cross validation folds `collect_metrics()`

```{r eval=FALSE}
set.seed(___)
gss16_folds <- vfold_cv(___, v = ___)

gss16_fit_rs_1 <- gss16_wflow_1 %>%
  fit_resamples(___)

collect_metrics(___, summarize = FALSE)
collect_metrics(___)
```

```{r 8-cv, message=FALSE, warning=FALSE}
library(tidymodels)

set.seed(2025)  
gss16_folds <- vfold_cv(gss16_train, v = 5)

gss16_fit_rs_1 <- gss16_wflow_1 %>%
  fit_resamples(
    resamples = gss16_folds,
    metrics   = metric_set(roc_auc, accuracy),
    control   = control_resamples(save_pred = TRUE)
  )

collect_metrics(gss16_fit_rs_1, summarize = FALSE)

collect_metrics(gss16_fit_rs_1)
```
***I observed that the average accuracy of the model on the training set was about 83.7%, and the average AUC was about 0.631. The standard errors of both indicators were very small, indicating that the performance on different folds was relatively consistent; however, the AUC was not particularly high, indicating that the model's ability to distinguish between "agree" and "disagree" could be improved.***


9.  Now, try a different, simpler model: predict `advfront` from only `polviews` and `educ`.
    Specifically,

    -   update the recipe to reflect this simpler model specification (and name it `gss16_rec_2`),
    -   redefine the workflow with the new recipe (and name this new workflow `gss16_wflow_2`),
    -   perform cross validation, and
    -   report the average area under the ROC curve and the accuracy for all cross validation folds `collect_metrics()`.
    
```{r 9-simple-model, message=FALSE, warning=FALSE}
library(tidymodels)

gss16_rec_2 <- recipe(advfront ~ polviews + educ, data = gss16_train) %>%
  step_dummy(all_nominal(), -all_outcomes())

gss16_wflow_2 <- workflow() %>%
  add_model(gss16_spec) %>%
  add_recipe(gss16_rec_2)

gss16_fit_rs_2 <- gss16_wflow_2 %>%
  fit_resamples(
    resamples = gss16_folds,
    metrics   = metric_set(roc_auc, accuracy),
    control   = control_resamples(save_pred = TRUE)
  )
collect_metrics(gss16_fit_rs_2, summarize = FALSE)

collect_metrics(gss16_fit_rs_2)
```

***I observed an average accuracy of about 83.8% and an average AUC of about 0.64. Compared with the previous model with wrkstat (accuracy 83.7%, AUC 0.63), it is slightly better. This shows that in this data, polviews and edu can capture most of the prediction information, but wrkstat does not bring any additional gain.***

10. Comment on which model performs better (one including `wrkstat`, model 1, or the one excluding `wrkstat`, model 2) on the training data based on area under the ROC curve.

- **Model 1** (with `wrkstat`): 0.631  
- **Model 2** (only `polviews` + `educ`): 0.639  

***The Model 2 has a slightly higher AUC, which means that the simplified model without `wrkstat` performs well in distinguishing "agree" from "disagree", and `wrkstat` still does not bring any additional improvement.***

11. Fit both models to the testing data, plot the ROC curves for the predictions for both models, and calculate the areas under the ROC curve.
    Does your answer to the previous exercise hold for the testing data as well?
    Explain your reasoning.
    Note: If you haven't yet done so, you'll need to first train your workflows on the training data with the following, and then use these fit objects to calculate predictions for the test data.

```{r eval=FALSE}
gss16_fit_1 <- gss16_wflow_1 %>%
  fit(gss16_train)

gss16_fit_2 <- gss16_wflow_2 %>%
  fit(gss16_train)
```

```{r}
library(tidymodels)
library(ggplot2)

gss16_fit_1 <- gss16_wflow_1 %>% fit(data = gss16_train)
gss16_fit_2 <- gss16_wflow_2 %>% fit(data = gss16_train)

test_probs_1 <- predict(gss16_fit_1, gss16_test, type = "prob") %>%
  bind_cols(
    gss16_test %>%
      select(advfront) %>%
      mutate(advfront = factor(advfront, levels = c("Agree", "Not agree")))
  )

test_probs_2 <- predict(gss16_fit_2, gss16_test, type = "prob") %>%
  bind_cols(
    gss16_test %>%
      select(advfront) %>%
      mutate(advfront = factor(advfront, levels = c("Agree", "Not agree")))
  )

roc1 <- roc_curve(test_probs_1, truth = advfront, .pred_Agree) %>% mutate(model = "with wrkstat")
roc2 <- roc_curve(test_probs_2, truth = advfront, .pred_Agree) %>% mutate(model = "no wrkstat")
roc_all <- bind_rows(roc1, roc2)

ggplot(roc_all, aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_path(size = 1) +
  labs(
    title = "ROC Curves on Test Set",
    x = "False Positive Rate\n(1 − Specificity)",
    y = "True Positive Rate\n(Sensitivity)",
    color = "Model"
  )

auc1 <- roc_auc(test_probs_1, truth = advfront, .pred_Agree)
auc2 <- roc_auc(test_probs_2, truth = advfront, .pred_Agree)

auc1
auc2
```
***I observed that in the ROC graph, the red line (no wrkstat) rushed faster in the leftmost section, indicating that it can catch more true positive samples with almost no errors. After the middle section, the blue line (with wrkstat) slightly increased, which means that if relax the error tolerance, it can catch more true positives. But overall, the area under the red line is larger (AUC≈0.619 vs. 0.615), so I think the simplified model without wrkstat has a stronger overall discrimination ability.***


🧶 ✅ ⬆️ Knit, *commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.*

## Harassment at work

In 2016, the GSS added a new question on harassment at work.
The question is phrased as the following.

> Over the past five years, have you been harassed by your superiors or co-workers at your job, for example, have you experienced any bullying, physical or psychological abuse?

Answers to this question are stored in the `harass5` variable in our dataset.

12. Create a subset of the data that only contains `Yes` and `No` answers for the harassment question. How many responses chose each of these answers?


```{r 12-harass-count, message=FALSE, warning=FALSE}
library(tidyverse)

harass_counts <- gss16 %>%
  filter(harass5 %in% c("Yes", "No")) %>%
  count(harass5)

harass_counts
```
**1136** respondents answered “No”, **237** respondents answered “Yes”

13. Describe how bootstrapping can be used to estimate the proportion of Americans who have been harassed by their superiors or co-workers at their job.

***I’ll use our filtered data as the “original” sample, then repeat these steps 2,000 times***
***1.Draw a bootstrap sample of the same size with replacement from the original data.***
***2.Compute the fraction of harass5 == "Yes" in that sample.***
***3.Save that proportion.***
***4.After looping, I’ll have a distribution of those proportions. I can then take its 2.5th and 97.5th percentiles to form a 95% confidence interval, or look at the distribution’s standard deviation as an estimate of uncertainty.***


```{r}
set.seed(2025)
B <- 2000
n    <- nrow(gss16 %>% filter(harass5 %in% c("Yes","No")))
data <- gss16 %>% filter(harass5 %in% c("Yes","No"))

boot_props <- replicate(B, {
  samp <- data[sample(seq_len(n), size = n, replace = TRUE), ]
  mean(samp$harass5 == "Yes")
})

quantile(boot_props, c(0.025, 0.975))
```

14. Calculate a 95% bootstrap confidence interval for the proportion of Americans who have been harassed by their superiors or co-workers at their job. Interpret this interval in context of the data.
***After 2,000 bootstrap resamplings, I found that the 95% confidence interval for the harassment rate is between 15.3% and 19.2%. In other words, I can be 95% sure that approximately 15.3%–19.2% of Americans have been harassed at work by their supervisors or colleagues.***

```{r}
quantile(boot_props, c(0.025, 0.975))
```

15. Would you expect a 90% confidence interval to be wider or narrower than the interval you calculated above? Explain your reasoning.

***If the confidence level is lowered to 90%, the confidence interval will become narrower, because the 90% CI only needs to cover the range from approximately the 5th to the 95th percentile, unlike the 95% CI which also extends to the 2.5th and 97.5th percentiles, so it does not have to include the most extreme values at both ends, and the interval is naturally smaller.***


🧶 ✅ ⬆️ Knit, *commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and review the md document on GitHub to make sure you're happy with the final state of your work.*
